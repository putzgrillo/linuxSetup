options(dplyr.summarise.inform = FALSE)
library(glmnet)
library(fitdistrplus)
library(fPortfolio)
library(tidyverse)    


# data steps: ----
# # data steps: import data ----
p1_df <- readxl::read_xlsx('C:\\Users\\00690\\Downloads\\HMS DSI Problem Set 2.xlsx',
                           sheet = "Problem1",
                           range = "A1:H10001", col_names = TRUE)

# # data steps: cost data ----
df_cost <- data.frame(campaign = letters[1:4],
                      cost = c(7, 5, 4, 2)/10)

# # data steps: generate a dataset with path of the customer ----
        # in this step the overall costs are summed cumulatively
        # later in the process I found out most of the transformations here were not needed
        # but it is a data structure that might be relevant if one wants to model the transitions between states 
        # where each marketing channel is a state 
df_path <- p1_df %>%
  # remove conversion and value to add later (only for last non NA click)
  select(-conversion, -value) %>%
  # turn into long format, keep until endpoints and change 'click_x' to numeric x
  pivot_longer(-user, names_to = "iteration", values_to = "campaign") %>%
  filter(!is.na(campaign)) %>%
  mutate(iteration = as.numeric(gsub('click_', '', iteration))) %>%
  # amount of campaigns (soon it will be replaced by their cumsum)
  mutate(
    campaign_a = as.numeric(campaign == 'a'),
    campaign_b = as.numeric(campaign == 'b'),
    campaign_c = as.numeric(campaign == 'c'),
    campaign_d = as.numeric(campaign == 'd')
  ) %>%
  # include generated costs
  left_join(., df_cost, by = "campaign") %>%
  # to each user
  group_by(user) %>%
  # do generate: cumulative cost, index of max_iteration, what was the previous campaign
    # max_iter index will be removed (just a temp key to attribute conversion & value)
  mutate(
    across(starts_with("campaign_"), .fns = cumsum),
    previous_campaign = ifelse(iteration > 1, lag(campaign, 1), "begin"),
    max_iter = max(iteration[!is.na(campaign)]),
    cost_cum = cumsum(cost)
  ) %>%
  # collect the conversion and value generated by each client
  left_join(., {p1_df %>% select(user, conversion, value)}, by = "user") %>%
  # attribute the conversion and value generated only to last non NA iteration of each customer
  mutate(
    conversion = ifelse(iteration == max_iter, conversion, 0),
    value = ifelse(iteration == max_iter, value, 0)
  ) 

# df_path %>% filter(user == 5)

# # data steps: verify frequency of campaigns in paths ----
        # in this step, the idea is to verify how many customer endpoints had at least one of the campaign touch-points
        # luckily, it is almost uniformely distributed along the combinations of possible campaigns
df_last_state %>%
  group_by(
    campaign_a = sign(campaign_a),
    campaign_b = sign(campaign_b),
    campaign_c = sign(campaign_c),
    campaign_d = sign(campaign_d)
  ) %>%
  summarise(freq = n())


# # data steps: data with the last observation of each customer (endpoint) ----
        # in this step the data with endpoint is created, selecting the 
        # last available observation for each customer
df_last_state <- df_path %>%
  arrange(-iteration) %>%
  group_by(user) %>%
  mutate(part_campaigns = paste(sort(unique(campaign)), collapse = ",")) %>%
  slice(1) %>% ungroup() 


# # data steps: generate all combinations of marketing ----
        # create a grid with all possible combinations of marketing mix
        # later it will be used to filter if customer received contact of a given combination
df_mix_comb <- 
  expand.grid(campaign_a = 0:1,
              campaign_b = 0:1,
              campaign_c = 0:1,
              campaign_d = 0:1) %>% 
  as.data.frame()
df_mix_comb <- df_mix_comb[apply(df_mix_comb, 1, sum) > 0, ]

# # data steps: start bootstrap to generate metrics of interest ----
        # in this step a huge amount of data is created to help infer the impact of each marketing channel
        # for every combination of marketing mix (customer was in a given combination of touchpoints)
            # generate 500 bootstraps sampling a fraction of the observations (small to create more robust CI)
            # for every bootstrap sampling, calculate:
                # conversion rate
                # mean value
                # mean cost
                # amount of clicks in each campaign
                # mean profit (value - cost)
                # mean return (profit / cost - 1) 
                # share of expenditure on each channel
n_bootstrap <- 500
df_combinations <- vector("list", nrow(df_mix_comb))

for (w in seq_along(df_combinations)) {
  # set current iteration
  temp_bootstrap <- vector("list", n_bootstrap)
  
  for (y in seq_along(temp_bootstrap)) {
    # summarise important variables given a sample
    temp_inboot <- df_last_state %>%
      filter(
        sign(campaign_a) == df_mix_comb$campaign_a[w]
        & sign(campaign_b) == df_mix_comb$campaign_b[w]
        & sign(campaign_c) == df_mix_comb$campaign_c[w]
        & sign(campaign_d) == df_mix_comb$campaign_d[w]
      ) %>% 
      sample_frac(0.3) %>%
      group_by(part_campaigns) %>%
      summarise(
        conv_rate = mean(conversion),                     # what is the conversion rate
        mean_value = mean(value),                         # what is the mean value (not omitting zero)
        mean_cost = mean(cost_cum),                       # average incurred cost (current)
        across(starts_with("campaign_"), .fns = sum)      # sum amount of clicks (temp to compute cost share)
      ) %>% ungroup() %>%
      mutate(
        mean_profit = mean_value - mean_cost,
        mean_return = mean_profit / mean_cost
      )
    
    # add a table with cost share per campaign 
    temp_share <- temp_inboot %>%
      select(starts_with("campaign_")) %>%
      pivot_longer(everything(), names_to = "campaign", values_to = "times") %>%
      mutate(campaign = gsub("campaign_", "", campaign))%>%
      left_join(., df_cost, by = "campaign") %>%
      mutate(cost = times * cost) %>%
      mutate(share = cost / sum(cost)) %>%
      select(campaign, share) %>%
      pivot_wider(names_from = campaign, values_from = share, 
                  names_prefix = "share_campaign_")
    
    # adjust campaign_ to binary
    temp_inboot <- temp_inboot %>%
      mutate(across(starts_with("campaign_"), .fns = sign))
    
    # join tables
    temp_bootstrap[[y]] <- bind_cols(temp_inboot, 
                                     temp_share)
    rm(list = c("temp_inboot", "temp_share"))
  }
  
  df_combinations[[w]] <- bind_rows(temp_bootstrap)
  rm(temp_bootstrap)
}

df_inference <- bind_rows(df_combinations)

p1_boxplot_bootstrap <- df_inference %>%
  select(part_campaigns, conv_rate, mean_profit, mean_return) %>%
  pivot_longer(-part_campaigns, names_to = "variable", values_to = "value") %>%
  ggplot(., aes(x = part_campaigns, y = value, colour = part_campaigns)) +
    geom_boxplot() +
    geom_hline(yintercept = 0, linetype = 3, size = 1.2) +
    facet_wrap(~variable, scales = "free", ncol = 1, strip.position = "left") +
    xlab("combination of campaigns (not in order)") +
    theme_bw()


# # data steps: create interaction terms to evaluate also pair-wise combination of channels ----
      # in this step the pairwise interaction is created 
interact_terms <- function(df) {
  # generate combinations
  x <- df 
  new_x <- t(apply(x, 1, combn, 2, prod))
  # adjust names (remove campaign to make it shorter)
  colnm <- gsub("campaign_", "", colnames(x))
  colnames(new_x) <- paste("interact_campaign_", combn(colnm, 2, paste, collapse = "_"), sep="")
return(new_x)
}

df_inference <- cbind(df_inference,
                      interact_terms(df_inference %>% select(starts_with("campaign_"))))



# inferece ----
# # inference: most successful in terms of units sales? ----
          # in this step the data generated from bootstrap is used to evaluate the questions
          # at first, since there are many variables (dummies for campaign, interaction terms and share of cost)
          # a regularization strategy is employed to determine if some variables can be removed

          # what is the conversion distribution?
fitdist(df_inference$conv_rate, distr = 'lnorm', method = 'mle') %>%
  gofstat()

p1_plot_distribution_conversion <- 
  ggplot(df_inference, aes(x = conv_rate)) +
  geom_histogram(aes(y = ..density..), fill = "lightblue", colour = "black") +
  scale_x_continuous(labels = scales::percent_format()) +
  xlab("Conversion Rate (%)") +
  theme_bw()


          # trying regularization 
dependent_variable <- as.matrix(df_inference$conv_rate)
independent_variable <- df_inference %>% 
  select(contains("campaign_")) %>%
  as.matrix()

cv_lasso <- cv.glmnet(y = dependent_variable,
                      x = independent_variable,
                      family = "gaussian",
                      alpha = 1,
                      nfolds = 15,
                      intercept = TRUE)

            # using LASSO, all the coefficients are kept
coef(cv_lasso, s = "lambda.1se")


          # running a straightforward regression with few parameters to increase interpretability
p1_model_conversion <- 
  glm(conv_rate ~ campaign_a + campaign_b + campaign_c + campaign_d + 
        interact_campaign_a_b + interact_campaign_a_c + interact_campaign_a_d + 
        interact_campaign_b_c + interact_campaign_b_d + interact_campaign_c_d,
      data = df_inference,
      family = "gaussian")

  
summary(p1_model_conversion)
hist(residuals(p1_model_conversion))
plot(p1_model_conversion, 2)





# # inference: return on investment on each campaign ----
            # what is the return distribution?
fitdist(df_inference$mean_return, distr = 't', method = 'mle') %>%
  gofstat()

p1_plot_distribution_return <- 
  ggplot(df_inference, aes(x = mean_return)) +
  geom_histogram(aes(y = ..density..), fill = "lightblue", colour = "black") +
  scale_x_continuous(labels = scales::percent_format()) +
  xlab("Simple Return (%)") +
  theme_bw()

            # model distribution
p1_model_return <- 
  glm(mean_return ~ campaign_a + campaign_b + campaign_c + campaign_d + 
        interact_campaign_a_b + interact_campaign_a_c + interact_campaign_a_d +
        interact_campaign_b_c + interact_campaign_b_d + interact_campaign_c_d,
      data = df_inference,
      family = "gaussian")

summary(p1_model_return)
hist(residuals(p1_model_return))
plot(p1_model_return, 2)



# # inference: how to allocate the cash on each campaign ----
p1_model_allocation <- 
  glm(mean_return ~ 0 + share_campaign_a + share_campaign_b + share_campaign_c + share_campaign_d,
      data = df_inference,
      family = "gaussian")

summary(p1_model_allocation)
hist(residuals(p1_model_allocation))
plot(p1_model_allocation, 2)



portfolio_spec <- fPortfolio::portfolioSpec(
  model = list(type = "CVaR",
               params = list(alpha = 0.05),
               tailRisk = list()),
  portfolio = list(
    targetReturn = target
  ),
  optim = list(
    solver = "solveRglpk.CVAR"
  )
)
# OTIMIZAÇÃO PORTFOLIO
portfolio_return <- as.timeSeries(next_day_returns)
portfolio_final <- fPortfolio::efficientPortfolio(data = portfolio_return,
                                                  spec = portfolio_spec)
# PESOS DO PORTFÓLIO
portfolio_weights <- portfolio_final@portfolio@portfolio$weights
portfolio_weights <- data.frame(t(portfolio_weights / sum(portfolio_weights)))

